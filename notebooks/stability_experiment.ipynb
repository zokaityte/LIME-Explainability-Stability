{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os.path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "from lime.metrics import jaccard_similarities"
   ],
   "id": "48709075cdc19245",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 1. Dataset preparation\n",
    "### 1.1 Define data"
   ],
   "id": "99def39918a2cb1a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Determine data paths\n",
    "\n",
    "DATA_DIR = '..\\data'\n",
    "DATASET_DIR = 'sample_dataset_1'\n",
    "MODELS_DIR = '..\\model_checkpoints'\n",
    "\n",
    "CATEGORICAL_FEATURES = [\"Dst Port\", 'Timestamp', 'Label', 'Protocol', 'Fwd PSH Flags', 'FIN Flag Cnt', 'SYN Flag Cnt',\n",
    "                        'RST Flag Cnt', 'PSH Flag Cnt', 'ACK Flag Cnt', 'URG Flag Cnt', 'ECE Flag Cnt',\n",
    "                        'Fwd Seg Size Min']\n",
    "\n",
    "# TODO Dropping these categorical features for now, need to handle them later. Leaving only binary categorical features.\n",
    "COLUMNS_TO_DROP = [\n",
    "    'Timestamp',  # Needs a way for enncoding (time of day?)\n",
    "    'Dst Port',\n",
    "    # Categories: [53, 80, 22, 21, 3389, 64458, 443, 50158, 445, 50243, 49730, 51240, 0, 55146, 49942, 50197, 41096, 51579, 49154, 54429, 52737, 49948, 50574, 52147, 49681, 10884, 56754, 44285, 50667, 5355, 50576, 51311, 51090, 63467, 55232, 50633, 50516, 50122, 50746, 51740, 30158, 51902, 49713, 50227, 137, 54698, 49615, 51189, 52165, 50865, 13056, 49552, 51182, 21462, 49480, 49739, 51392, 25603, 49867, 49886, 60827, 51966, 54706, 49870, 50900, 52013, 50608, 50228, 51324, 51445, 50074, 2046, 50073, 55189, 63979, 50376, 38872, 4899, 49995, 50593, 62410]\n",
    "    'Protocol',  # Categories: [17, 6, 0] (one-hot ?)\n",
    "    'Fwd Seg Size Min'  # Categories: [8, 20, 32, 40, 0, 28] (one-hot ?)\n",
    "]\n",
    "CATEGORICAL_FEATURES = [col for col in CATEGORICAL_FEATURES if col not in COLUMNS_TO_DROP]\n",
    "# TODO now dropping rows with infinity values, need to handle them later\n",
    "\n",
    "train_data_path = os.path.join(DATA_DIR, DATASET_DIR, 'train_data.csv')\n",
    "val_data_path = os.path.join(DATA_DIR, DATASET_DIR, 'val_data.csv')\n",
    "test_data_path = os.path.join(DATA_DIR, DATASET_DIR, 'test_data.csv')"
   ],
   "id": "46d9fd149a7bebc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1. 2 Load data and encode labels",
   "id": "2650eb765001549a"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "# Load dataset\n",
    "train_data = pd.read_csv(train_data_path).drop(columns=COLUMNS_TO_DROP)\n",
    "val_data = pd.read_csv(val_data_path).drop(columns=COLUMNS_TO_DROP)\n",
    "test_data = pd.read_csv(test_data_path).drop(columns=COLUMNS_TO_DROP)\n",
    "\n",
    "\n",
    "# Drop rows with infinity values\n",
    "def drop_infinity_rows(df):\n",
    "    return df[~df.isin([np.inf, -np.inf]).any(axis=1)]\n",
    "\n",
    "\n",
    "# Apply the function to each dataset\n",
    "train_data = drop_infinity_rows(train_data)\n",
    "val_data = drop_infinity_rows(val_data)\n",
    "test_data = drop_infinity_rows(test_data)\n",
    "\n",
    "# Split features and labels\n",
    "train_x, train_y = train_data.iloc[:, :-1], train_data.iloc[:, -1]\n",
    "val_x, val_y = val_data.iloc[:, :-1], val_data.iloc[:, -1]\n",
    "test_x, test_y = test_data.iloc[:, :-1], test_data.iloc[:, -1]\n",
    "\n",
    "# Setup encoder for labels\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(pd.concat([train_y, val_y, test_y]))\n",
    "class_names = label_encoder.classes_\n",
    "\n",
    "# Encode labels\n",
    "train_y = label_encoder.transform(train_y)\n",
    "val_y = label_encoder.transform(val_y)\n",
    "test_y = label_encoder.transform(test_y)\n",
    "\n",
    "# Turn to numpy arrays\n",
    "train_x_np = train_x.to_numpy()\n",
    "val_x_np = val_x.to_numpy()\n",
    "test_x_np = test_x.to_numpy()\n",
    "\n",
    "print(\"Created dataset (train, val, test):\", train_x.shape, val_x.shape, test_x.shape)\n",
    "print(\"Y labels:\", class_names)"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.3 Check feature information",
   "id": "5574e67897427b60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Feature names\n",
    "feature_names = train_x.columns\n",
    "\n",
    "# Categorical features information extraction\n",
    "categorical_columns_dict = {\n",
    "    train_x.columns.get_loc(col): train_x[col].unique().tolist()\n",
    "    for col in CATEGORICAL_FEATURES if col in train_x\n",
    "}\n",
    "\n",
    "continuous_columns = [col for col in train_x.columns if col not in CATEGORICAL_FEATURES]\n",
    "\n",
    "# Feature summary\n",
    "print(f\"Total number of features: {len(train_x.columns)}\\n\")\n",
    "\n",
    "# Categorical columns with their indices, names, and categories\n",
    "print(f\"Categorical columns ({len(categorical_columns_dict)}):\")\n",
    "for col_index, categories in categorical_columns_dict.items():\n",
    "    col_name = train_x.columns[int(col_index)]  # Ensure col_index is treated as integer\n",
    "    print(f\"  - Index: {col_index}, Name: {col_name}, Categories: {categories}\")\n",
    "\n",
    "# Continuous columns with their indices and names\n",
    "print(f\"\\nContinuous columns ({len(continuous_columns)}):\")\n",
    "for col in continuous_columns:\n",
    "    col_index = train_x.columns.get_loc(col)\n",
    "    print(f\"  - Index: {col_index}, Name: {col}\")\n"
   ],
   "id": "845cd87f544d4b9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Output: \n",
    "1. train, validation, and test data (x: train_x, val_x, test_x), their encoded labels(y)\n",
    "2. feature information - categorical features and their categories, all features names, label names."
   ],
   "id": "fe1f9af2d9e42987"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 2. Model training\n",
    "\n",
    "### 2.1 Random Forest Classifier"
   ],
   "id": "366f95d2cc7e2c2c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "class RandomForestModel:\n",
    "    def __init__(self, n_estimators, random_state, max_depth, max_features):\n",
    "        self.model = RandomForestClassifier(n_estimators=n_estimators, random_state=random_state, max_depth=max_depth,\n",
    "                                            max_features=max_features)\n",
    "\n",
    "    def train(self, train_x, train_y):\n",
    "        self.model.fit(train_x, train_y)\n",
    "\n",
    "    def evaluate(self, test_x, test_y):\n",
    "        accuracy = self.model.score(test_x, test_y)\n",
    "        print(\"Test accuracy:\", accuracy)\n",
    "        return accuracy\n",
    "\n",
    "    def save(self, path):\n",
    "        joblib.dump(self.model, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        self.model = joblib.load(path)"
   ],
   "id": "6e9840bef092759f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rf_model = RandomForestModel(n_estimators=10, random_state=0, max_depth=5, max_features=5)\n",
    "\n",
    "# Load model if exists, train and save otherwise\n",
    "checkpoint_dir = f'{MODELS_DIR}/{DATASET_DIR}_rf.pkl'\n",
    "\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    rf_model.load(checkpoint_dir)\n",
    "else:\n",
    "    rf_model.train(train_x_np, train_y)\n",
    "    rf_model.evaluate(val_x_np, val_y)\n",
    "    rf_model.save(checkpoint_dir)\n",
    "\n",
    "model_to_explain = rf_model.model"
   ],
   "id": "be48f69ce2eb70bb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Output: a trained model",
   "id": "600205ba33615746"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 3. Model explanation",
   "id": "81efabb9f356acba"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define explainability configuration\n",
    "\n",
    "experiment_configuration = {\n",
    "    \"dataset\": DATASET_DIR,\n",
    "    \"number_of_features\": len(train_x.columns),\n",
    "    \"number_of_categorical_features\": len(categorical_columns_dict),\n",
    "    \"number_of_continuous_features\": len(continuous_columns),\n",
    "    \"model\": 'RandomForest',\n",
    "    \"times_explained\": 20,\n",
    "    \"kernel_width\": None,\n",
    "    \"kernel\": None,\n",
    "    \"verbose\": False,\n",
    "    \"feature_selection\": 'auto',\n",
    "    \"sample_around_instance\": False,\n",
    "    \"num_features\": 10,\n",
    "    \"num_samples\": 5000,\n",
    "    \"distance_metric\": 'euclidean',\n",
    "    \"model_regresor\": None,\n",
    "    \"sampling_func\": 'pareto'\n",
    "}\n",
    "\n",
    "# Define test instance\n",
    "random_test_index = np.random.randint(0, test_x_np.shape[0])\n",
    "test_x_sample = test_x_np[random_test_index]\n",
    "test_y_sample = test_y[random_test_index]"
   ],
   "id": "9a85b130bd424fba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load explainer\n",
    "\n",
    "categorical_features = list(categorical_columns_dict.keys())\n",
    "\n",
    "explainer = LimeTabularExplainer(\n",
    "    training_data=train_x_np,\n",
    "    mode=\"classification\",\n",
    "    feature_names=feature_names,\n",
    "    categorical_features=categorical_columns_dict.keys(),\n",
    "    categorical_names=categorical_columns_dict,\n",
    "    class_names=class_names,\n",
    "    kernel_width=experiment_configuration['kernel_width'],\n",
    "    kernel=experiment_configuration['kernel'],\n",
    "    verbose=experiment_configuration['verbose'],\n",
    "    feature_selection=experiment_configuration['feature_selection'],\n",
    "    sample_around_instance=experiment_configuration['sample_around_instance'],\n",
    "    random_state=None\n",
    ")\n",
    "\n",
    "explainer.set_sampling_func(experiment_configuration['sampling_func'])\n",
    "\n",
    "experiment_results = {label: [] for label in class_names}\n",
    "\n",
    "# Explain the same instance multiple times\n",
    "for i in range(experiment_configuration[\"times_explained\"]):\n",
    "    exp = explainer.explain_instance(\n",
    "        data_row=test_x_sample,\n",
    "        predict_fn=model_to_explain.predict_proba,\n",
    "        top_labels=len(class_names),  # number of labels to explain. Explaining all\n",
    "        num_features=experiment_configuration[\"num_features\"],  # maximum number of features present in explanation\n",
    "        num_samples=experiment_configuration[\"num_samples\"],  # size of the neighborhood to learn the linear model\n",
    "        distance_metric=experiment_configuration[\"distance_metric\"],  # the distance metric to use for weights.\n",
    "        model_regressor=experiment_configuration[\"model_regresor\"],\n",
    "        # surrogate model / defaults to Ridge regression in LimeBase\n",
    "    )\n",
    "\n",
    "    # Append explanation to the corresponding label\n",
    "    for label_index, label in enumerate(class_names):\n",
    "        experiment_results[label].append(exp.as_list(label_index))\n",
    "\n",
    "# Show last explanation\n",
    "for label_index, label in enumerate(class_names):\n",
    "    exp.as_pyplot_figure(label_index)\n",
    "\n",
    "# Calculate stability of explanations\n",
    "stability_results = {}\n",
    "for label in class_names:\n",
    "    explanations = experiment_results[label]\n",
    "    explanations_features = [[f[0] for f in exp] for exp in explanations]\n",
    "    sim = jaccard_similarities(explanations_features)\n",
    "    stability_results[\"Stability \" + label] = np.asarray(sim).mean()\n",
    "\n",
    "results_dict = {}\n",
    "results_dict.update(experiment_configuration)\n",
    "results_dict.update(stability_results)\n",
    "\n",
    "results = pd.DataFrame(results_dict, index=[0])\n",
    "results.to_csv(f'results.csv', index=False)\n",
    "\n",
    "results"
   ],
   "id": "a6726967f4a9c6ac",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
